{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchmetrics.functional import pairwise_cosine_similarity\n",
    "from datasets import Dataset\n",
    "from sentence_transformers import SentenceTransformer, losses\n",
    "from sentence_transformers.trainer import SentenceTransformerTrainer, SentenceTransformerTrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triples(train_data, label_column, id_column, topk=10):\n",
    "    \n",
    "    # get X and y data (in aligned order)\n",
    "    X = list(train_data['embeddings'])\n",
    "    y = list(train_data[label_column])\n",
    "    ids = list(train_data[id_column])\n",
    "    assert set(y) == {0, 1}\n",
    "\n",
    "    # store embeddings of positive and negative examples and their ids\n",
    "    pos_vecs, neg_vecs, pos_data_ids, neg_data_ids = [], [], [], []\n",
    "    for i, y in enumerate(y):\n",
    "        if y == 1:\n",
    "            pos_vecs.append(X[i])\n",
    "            pos_data_ids.append(ids[i])\n",
    "        elif y == 0:\n",
    "            neg_vecs.append(X[i])\n",
    "            neg_data_ids.append(ids[i])\n",
    "    \n",
    "    # compute pairwise cosine similarity matrix between positive and negative embeddings\n",
    "    posneg_pairwise_sim = pairwise_cosine_similarity(torch.stack(pos_vecs), torch.stack(neg_vecs))\n",
    "    # map data ids to ranked ids of negative examples (based on cosine similarity)\n",
    "    posid2sortednegids = {pos_data_ids[p_id]: [neg_data_ids[x] for x in np.argsort(posneg_pairwise_sim[p_id])] for p_id in range(len(pos_vecs))}\n",
    "    # compute pairwise cosine similarity matrix between positive embeddings\n",
    "    pospos_pairwise_sim = pairwise_cosine_similarity(torch.stack(pos_vecs), torch.stack(pos_vecs))\n",
    "    # map data ids to ranked ids of positive examples (based on cosine similarity)\n",
    "    posid2sortedposids = {pos_data_ids[p_id]: [pos_data_ids[x] for x in np.argsort(pospos_pairwise_sim[p_id])] for p_id in range(len(pos_vecs))}\n",
    "\n",
    "    # collect a set of triples for every positive embedding as anchor embedding\n",
    "    sample_size = min([topk, len(pos_vecs), len(neg_vecs)])\n",
    "    triples = []\n",
    "    for anchor_id in pos_data_ids:\n",
    "        # triple positive: top k positive embeddings that are most similar to anchor embedding\n",
    "        for i in range(sample_size):\n",
    "            pos_id = posid2sortedposids[anchor_id][-(i+1)]\n",
    "            if anchor_id != pos_id:\n",
    "                for j in range(sample_size):\n",
    "                    # triple negative: top k negative embeddings that are most similar to anchor embedding\n",
    "                    neg_id = posid2sortednegids[anchor_id][-(j+1)]\n",
    "                    triples.append((anchor_id, pos_id, neg_id))\n",
    "\n",
    "    # returns list of triples as tuples of data ids\n",
    "    print('Resulting number of triples:', len(triples))\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_contrastive_model(train_data, model_dir, pretrained_model_name, batch_size=16, epochs=3, triplet_margin=1):\n",
    "    \n",
    "    train_dataset = Dataset.from_dict({\"anchor\": [t[0] for t in train_data], \"positive\": [t[1] for t in train_data], \"negative\": [t[2] for t in train_data]})\n",
    "    model = SentenceTransformer(pretrained_model_name).cpu() # if device is mps, because that doesn't work\n",
    "    train_loss = losses.TripletLoss(model=model, triplet_margin=triplet_margin)\n",
    "    args = SentenceTransformerTrainingArguments(output_dir=model_dir, per_device_train_batch_size=batch_size, num_train_epochs=epochs)\n",
    "    trainer = SentenceTransformerTrainer(model=model, args=args, train_dataset=train_dataset, loss=train_loss) #evaluator=evaluator\n",
    "    trainer.train()\n",
    "    model.save_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info on datafile and pre-trained model\n",
    "input_path = 'HateWiC_T5Defs_MajorityLabels.csv'\n",
    "id_column = 'id'\n",
    "sentence_column = 'T5generated_definition'\n",
    "label_column = 'majority_binary_annotation'\n",
    "\n",
    "pretrained_model_name = 'sentence-transformers/all-mpnet-base-v2'\n",
    "trained_model_dir = 'CL-model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = pd.read_csv(input_path, sep=';')\n",
    "model = SentenceTransformer(pretrained_model_name).cpu() # device='mps' gives error\n",
    "\n",
    "print('Encoding sentences with Sentence Transformer...')\n",
    "data['embeddings'] = list(model.encode(data[sentence_column], convert_to_tensor=True, show_progress_bar=True))\n",
    "\n",
    "train_data, dev_test_data = train_test_split(data, train_size=0.8, random_state=12)\n",
    "dev_data, test_data = train_test_split(dev_test_data, train_size=0.5, random_state=12)\n",
    "#print(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample training triples and train model with contrastive learning\n",
    "id_triples = sample_triples(train_data, label_column, id_column)\n",
    "id2sentence = {data_id: sent.lower() for data_id, sent in zip(data[id_column], data[sentence_column])}\n",
    "sentence_triples = [[id2sentence[id1], id2sentence[id2], id2sentence[id3]] for (id1, id2, id3) in id_triples]\n",
    "train_contrastive_model(sentence_triples, trained_model_dir, pretrained_model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
